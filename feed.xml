<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fmcgeough.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fmcgeough.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-27T13:10:18+00:00</updated><id>https://fmcgeough.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Postgresql - Optimizing SQL Performance</title><link href="https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance/" rel="alternate" type="text/html" title="Postgresql - Optimizing SQL Performance"/><published>2014-08-31T18:08:30+00:00</published><updated>2014-08-31T18:08:30+00:00</updated><id>https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance/"><![CDATA[<p>In a standard smallish Postgresql installation its actually fairly straightforward to figure out what indexes to create to eliminate sequential scans and improve your performance. But in a larger system where there are thousands of different queries, perhaps written by dozens of different engineers, the problem of addressing performance issues gets a bit more difficult. However, the same techniques can be used on both systems. I’ll describe what I use and perhaps it will be useful for someone else.</p> <h2 id="pgbadger">pgbadger</h2> <p>pgbadger is a <a href="https://github.com/dalibo/pgbadger">Postgresql log analyzer</a>. You should have it installed and setup a cron job to analyze your log every hour and send you a report. This is sort of a default thing to do on any Postgresql installation. It gives some nice general information.</p> <p>In order to make use of it you’ll need to setup your Postgresql installation to actually log. There are settings in postgresql.conf that will need to be configured in the “ERROR REPORTING AND LOGGING” section. Here is what I use :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log_destination = 'stderr'
logging_collector = on
log_filename = 'postgresql-%Y%m%d-%H.log'
log_truncate_on_rotation = on
log_rotation_age = 60min
log_rotation_size = 0
log_directory = '/postgres/tracelogs/pgsql'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d %h '
</code></pre></div></div> <p>I use pgbadger from a shell script (badger_reporting.sh) that has a single line in it :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># have pgbadger parse our Postgresql log
/usr/local/bin/pgbadger --prefix '%t [%p]: [%l-1] user=%u,db=%d %h ' --outfile $@
#
</code></pre></div></div> <p>Note: you’ll want the line you pass into pgbadger to mirror the output defined in your <code class="language-plaintext highlighter-rouge">log_line_prefix</code> definition. You are giving pgbadger the information that it needs to properly parse your log file.</p> <p>This shell script is called from another that passes in the name of the file to generate and the log file as source like this :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>badger_reporting.sh $LOCAL_HTML_OUTFILE $LOCAL_LOG_FILE
</code></pre></div></div> <p>Since we’re going to send an email to ourselves with our cron job its also nice to grep the postgresql log to see if there are ERRORS in it and send those in the body of the email so its immediately obvious. I do this with :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo "Errors in log" &gt; $LOCAL_LOG_FILE".err"
grep "ERROR:" $LOCAL_LOG_FILE &gt;&gt; $LOCAL_LOG_FILE".err"
</code></pre></div></div> <p>The mailing on our system is handled by mutt.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># mail out the pgbadger output
cat $LOCAL_LOG_FILE".err" | mutt -s "$SUBJECT" -a "$LOCAL_HTML_OUTFILE" -- "$RECIP_LIST"
#
</code></pre></div></div> <p>There are a lot of sections in the pgbadger report. The top of the report has “Overall Statistics”. You might want to familiarize yourself with the numbers that show up here and look for things that are out of the ordinary for the same day and time in your system. I also tend to look at “Queries that took up the most time” section.</p> <p>Anyway, pgbadger is a useful tool. Its free and fairly easy to setup. On most systems it provides at least some insight.</p> <p>You should realize that on most systems you’ll also have to set the <code class="language-plaintext highlighter-rouge">log_min_duration_statement</code> to some value that keeps your log from exploding in size. This means that pgbadger will have a whole slew of statements (ordinarily the bulk of them) that it won’t know anything about. Keep this in mind. There are other tools you can add to your arsenal that provide more full insight including the one I’ll describe next.</p> <h2 id="pg-stat-statements">pg stat statements</h2> <p>This should be required for any production Postgresql system (it actually is turned on if you use Amazon RDS version of Postgresql). If you come to a new system that doesn’t have it loaded this should be one of the first things that you address. While you’re at it add auto_explain to the set of things that get pre-loaded by postgresql. This is done by editing your postgresq.conf file and doing this :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># modify what preload libraries Postgresql uses
shared_preload_libraries = 'auto_explain,pg_stat_statements' # (change requires restart)
</code></pre></div></div> <p>As noted in the awesome postgresql.conf doc that accompanies the file, a change to this line requires a Postgresql restart. Do it. Then run the following from psql :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create extension pg_stat_statements;
</code></pre></div></div> <p>You are now much more awesome. As noted on the Postgresql online doc : The <code class="language-plaintext highlighter-rouge">pg_stat_statements</code> module provides a means for tracking execution statistics of all SQL statements executed by a server. In a sense this module is like a super-sized pgbadger. It gives you information that is immensely valuable. Read all about it on web at : <a href="http://www.postgresql.org/docs/9.3/static/pgstatstatements.html">pg_stat_statements</a> (change URL for whatever version you happen to be using).</p> <p>The default number of statements that are tracked is 1,000. Personally I’d bump that up to 10x by setting the following parameter in postgresql.conf file.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pg_stat_statements.max = 10000
</code></pre></div></div> <p>But this is really dependent upon your system and how many different types of queries you have hitting your database.</p> <p>The important queries to know about <code class="language-plaintext highlighter-rouge">pg_stat_statements</code> are :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-- reset statements. Empty everything out and start regathering
SELECT pg_stat_statements_reset();

-- which queries were called the most
SELECT * FROM pg_stat_statements ORDER BY calls desc LIMIT 50;

-- which queries used the most CPU time
SELECT * FROM pg_stat_statements ORDER BY total_time desc LIMIT 50;

-- what are the top 50 queries that I should look at (by total time hit)?
SELECT query,
calls,
total_time,
rows,
100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,
ROUND((total_time::numeric / calls::numeric),2) as average_time_per_call
FROM pg_stat_statements
ORDER BY total_time
DESC LIMIT 50;

-- what are the top 50 queries that I should look at (by speed of execution) ?
SELECT query,
calls,
total_time,
rows,
100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,
ROUND((total_time::numeric / calls::numeric),2) as average_time_per_call
FROM pg_stat_statements
ORDER BY 6
DESC LIMIT 50;
</code></pre></div></div> <p>How often should you run : <code class="language-plaintext highlighter-rouge">pg_stat_statements_reset</code>? Its really totally dependent upon your system. Here’s what I do. Every hour I grab the contents of pg_stat_statements and mail it to myself. Then I run pg_stat_statements_reset. This gives me a history and an hourly view of what is going on in the database.</p> <h2 id="pg-stat-user-tables">pg stat user tables</h2> <p>The <code class="language-plaintext highlighter-rouge">pg_stat_user_tables</code> contains a wealth of statistical information at a low level about your app tables. It is where to go to see if you are consistently sequentially scanning your million row table (whoops!) just because you missed adding an index after an application query change. Since its a running total I recommend taking a snapshot of the data, storing it in a temporary table and then taking another snapshot and diff’ing the two in order to see what happened over the period of time between your two snapshots. I actually have our system setup so this happens every hour during weekdays and business hours. I keep about a month worth of these snapshots in the temporary table. This lets me easily go back to a previous week (or two) and see if today’s traffic is significantly different from its corresponding traffic on same day at a point in the past.</p> <p>To create the temporary table perform the following SQL :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create table public.history_of_pg_stat as
SELECT CURRENT_TIMESTAMP as time_captured, *
FROM pg_stat_user_tables;
</code></pre></div></div> <p>Then periodically do the following :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>insert into public.history_of_pg_stat
SELECT CURRENT_TIMESTAMP, *
FROM pg_stat_user_tables;
</code></pre></div></div> <p>Now you can diff the two captured rows by :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with most_recent_time as
(
SELECT time_captured
FROM public.history_of_pg_stat
ORDER BY 1 DESC LIMIT 1
)
,
second_time as
(
SELECT time_captured
FROM public.history_of_pg_stat
WHERE time_captured &lt; (SELECT time_captured FROM most_recent_time)
ORDER BY 1 DESC
LIMIT 1
)
,
hops2 as
(
SELECT * FROM public.history_of_pg_stat
WHERE time_captured = (SELECT time_captured FROM most_recent_time)
),
stats as
(
SELECT to_char(hops.time_captured at TIME ZONE 'EST5DT', 'HH24:MI:SS') || ' - ' || to_char(hops2.time_captured at TIME ZONE 'EST5DT' , 'HH24:MI:SS') as capture_times,
hops2.time_captured-hops.time_captured as time_range,
hops2.schemaname || '.' || hops2.relname as tablename,
hops2.seq_scan - hops.seq_scan as seq_scans,
hops2.seq_tup_read - hops.seq_tup_read as seq_tup_read,
hops2.idx_scan - hops.idx_scan as idx_scans,
hops2.idx_tup_fetch - hops.idx_tup_fetch as idx_tup_fetch,
hops2.n_tup_ins - hops.n_tup_ins as n_tup_ins,
hops2.n_tup_upd - hops.n_tup_upd as n_tup_upd,
hops2.n_live_tup - hops.n_live_tup as n_new_live_tups,
hops2.n_live_tup as current_live_tup
FROM public.history_of_pg_stat hops
JOIN hops2 ON (hops.schemaname = hops2.schemaname and hops.relname = hops2.relname)
WHERE hops.time_captured = (SELECT time_captured FROM second_time)
)
SELECT * FROM stats
WHERE (seq_scans &gt; 0 OR idx_scans &gt; 0)
AND tablename NOT LIKE 'public.history_of_pg_stat'
AND current_live_tup &gt; 5000
ORDER BY 5 DESC;
</code></pre></div></div> <p>I hope some of this sketched out advice helps someone else in tracking down performance problems or better administer their system. Happy hunting!</p>]]></content><author><name></name></author><category term="SQL"/><category term="postgresql"/><category term="SQL"/><category term="postgresql"/><summary type="html"><![CDATA[SQL Optimization with Postgresql]]></summary></entry><entry><title type="html">Postgresql Connection Info</title><link href="https://fmcgeough.github.io/blog/2014/postgresql-connection-info/" rel="alternate" type="text/html" title="Postgresql Connection Info"/><published>2014-04-22T15:36:50+00:00</published><updated>2014-04-22T15:36:50+00:00</updated><id>https://fmcgeough.github.io/blog/2014/postgresql-connection-info</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/postgresql-connection-info/"><![CDATA[<p>Some days you just want to know who is connecting up to your database. The following SQL works for 9.3 Postgresql.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT COUNT(1) as num_connections,
COUNT(CASE WHEN s."state" = 'idle' THEN 1 ELSE NULL END) as num_idle_connections,
COUNT(CASE WHEN s."state" = 'idle in transaction' THEN 1 ELSE NULL END) as num_idle_in_tx_connections,
max(CASE WHEN s."state" = 'idle in transaction' THEN GREATEST(now(),s.query_start)-s.query_start ELSE NULL END) as age_of_oldest_tx,
s.datname,
s.client_addr
FROM pg_stat_activity s
WHERE s.pid != pg_backend_pid()
and s.client_addr is not null
GROUP BY s.client_addr, s.datname
ORDER BY 1 DESC;
</code></pre></div></div> <p>This query shows you total connections from by ip address along with some info that I find helpful - namely how many connections are connected but idle and how many are idle in transactions (and if so for how long).</p>]]></content><author><name></name></author><category term="database"/><category term="postgresql"/><summary type="html"><![CDATA[Who is connected to my Postgres database?]]></summary></entry><entry><title type="html">Moroccan Style Brisket</title><link href="https://fmcgeough.github.io/blog/2014/moroccan-style-brisket/" rel="alternate" type="text/html" title="Moroccan Style Brisket"/><published>2014-04-17T08:53:13+00:00</published><updated>2014-04-17T08:53:13+00:00</updated><id>https://fmcgeough.github.io/blog/2014/moroccan-style-brisket</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/moroccan-style-brisket/"><![CDATA[<p>Frankly I don’t think they necessarily have brisket in Morocco. Never been. and Moroccan-style is probably a stretch too but I had to call it something. Ah well. It was quite yummy and I got asked for recipe. Ingredients hmmm…let’s see.</p> <ul> <li>about 2 pound brisket</li> <li>one large red onion, sliced thin</li> <li>3 carrots, cleaned &amp; chopped</li> <li>2 parsnips, cleaned &amp; chopped</li> <li>2 TBL tomato paste</li> <li>1 TBL Tukas hot pepper paste</li> <li>1 tsp sweet paprika</li> <li>1 tsp ground cumin</li> <li>1/2 tsp ground ginger</li> <li>1/4 tsp cinnamon</li> <li>1 can chickpeas (drained and rinsed)</li> <li>2-3 cups beef broth</li> <li>couple handfuls of prunes cut in half</li> <li>olive oil</li> <li>salt &amp; pepper</li> <li>slivered almonds</li> </ul> <p>Now… what did I do?</p> <p>I turned my oven on low (300 degrees).</p> <p>I got a good sized Dutch oven (3 1/2 quart I think) and added a generous amount of olive oil over medium-high heat. I added the red onions and cooked them for about 6-7 minutes until they were quite soft. I added the spices and tomato paste and Tukas hot pepper paste and cooked it for about a minute. Then I threw in the brisket, followed by all the other ingredients (except for slivered almonds) and covered the dutch oven and put it in oven for about 3 hours.</p> <p>The house smelled wonderful at the end. I heated a bit more olive oil in a non-stick pan and threw in the slivered almonds with some sea salt. Toasted them for a couple minutes.</p> <p>Slice the brisket. Cover it with a bit of the veggies and delicious broth, sprinkle with slivered almonds. There ya go.</p>]]></content><author><name></name></author><category term="food"/><category term="recipes"/><summary type="html"><![CDATA[Brisket Recipe]]></summary></entry></feed>