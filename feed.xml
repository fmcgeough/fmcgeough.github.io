<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://fmcgeough.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://fmcgeough.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-05-27T13:43:16+00:00</updated><id>https://fmcgeough.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Programming Elixir</title><link href="https://fmcgeough.github.io/blog/2016/programming-elixir/" rel="alternate" type="text/html" title="Programming Elixir"/><published>2016-07-17T08:53:13+00:00</published><updated>2016-07-17T08:53:13+00:00</updated><id>https://fmcgeough.github.io/blog/2016/programming-elixir</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2016/programming-elixir/"><![CDATA[<p>For learning Elixir I started with the ubiquitous Dave Thomas and “Programming Elixir”. I’m bouncing back and forth between reading through that book and watching the ConFreaks videos of the last two year’s Elixir conferences (and thinking about whether I want to pay to go to this year’s conference which is in Orlando).</p> <p>“Programming Elixir” has been pretty good. The exercises at the end of the chapter cover the concepts discussed. I was stumped for a couple of minutes looking at “define a function head with the defaults”. But after working through the example it made sense and resulted in code that seemed to make sense to me.</p> <p>The thing is in Elixir (and Erlang) the function head (declaration) is a separate entity from the function body. Which is an odd thing to wrap your head around if you come from almost any other language background. In the case of the example with default parameters in the book you can do this :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    defmodule DefaultParams1 do
      def func(p1, p2 \\ 123)

      def func(p1, 99) do
        IO.puts "you said 99"
      end

      def func(p1, p2) do
        IO.inspect [p1, p2]
      end
    end
</code></pre></div></div> <p>If you call DefaultParams1.func(1) then the match will be on that first func without a body. The second parameter is added as a default and the search for a match continues. Obviously a contrived example for the book but I do like the approach of separating the definition of what we might want to supply as defaults from the rest of the code.</p> <p>Just beginning the journey into Elixir but its very interesting so far.</p>]]></content><author><name></name></author><category term="Elixir"/><category term="Elixir"/><summary type="html"><![CDATA[Learning Elixir]]></summary></entry><entry><title type="html">Learning Elixir and Phoenix</title><link href="https://fmcgeough.github.io/blog/2016/elixir-phoenix-ecto/" rel="alternate" type="text/html" title="Learning Elixir and Phoenix"/><published>2016-07-16T08:53:13+00:00</published><updated>2016-07-16T08:53:13+00:00</updated><id>https://fmcgeough.github.io/blog/2016/elixir-phoenix-ecto</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2016/elixir-phoenix-ecto/"><![CDATA[<p>Wow. This is the first time in a while that I’ve been excited about a new programming language. Where learning the language wasn’t just a chore to stay employed. Going through the documentation, code examples and a couple of recent books. Such a fundamentally different way of looking at solving problems. Using <a href="http://confreaks.tv/events/elixirconf2014">Confreaks.tv</a>, <a href="http://safaribooksonline.com">Safari Books online</a>, and the excellent documentation. Can’t wait to build something for real!</p>]]></content><author><name></name></author><category term="Elixir"/><category term="Elixir"/><summary type="html"><![CDATA[Learning Elixir]]></summary></entry><entry><title type="html">Postgresql - Lag and Windowing Function</title><link href="https://fmcgeough.github.io/blog/2015/postgresql-lag-and-windowing-function/" rel="alternate" type="text/html" title="Postgresql - Lag and Windowing Function"/><published>2015-05-07T22:03:34+00:00</published><updated>2015-05-07T22:03:34+00:00</updated><id>https://fmcgeough.github.io/blog/2015/postgresql-lag-and-windowing-function</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2015/postgresql-lag-and-windowing-function/"><![CDATA[<p>Lag came in handy again.</p> <p>Problem: customer stored their account info in an account_data table and recorded history of the changes to this table in an account_history table along with a new column dt_account_history_updated. Groovy. The way the account_history table was written was :</p> <ul> <li>on insert - write the values to account_history</li> <li>on update - write only the old values to account_history, but only for the column values that changes (using NULLIF function).</li> <li>on delete - write values to account_history. hmmm. so what you are looking at in account_history is the previous values. not the current ones (for update anyway). Got it?</li> </ul> <p>So… each customer has a status associated with it (ACTIVE, CANCELLED, SUSPENDED, etc). The question is how could you report when a customer was in that status if the account_history has only the previous value? Well. actually pretty easy using LAG.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with set_of_data as
(
SELECT ah.account_id, ah.dt_last_history_updated as dt_info, ah.status
FROM account_history ah
WHERE ah.status IS NOT NULL
UNION
SELECT acct.account_id, now(), acct.status
FROM account_data acct
),
build_data_set as
(
SELECT account_id, dt_info, status, LAG(dt_info, 1) OVER (PARTITION BY account_id ORDER BY dt_info ASC) as lag
FROM set_of_data
)
SELECT account_id,
status,
timezone('GMT'::text, lag) as dt_status
FROM build_data_set
WHERE lag IS NOT NULL
ORDER BY 1, 3;
</code></pre></div></div> <p>tada! We get all the data from the history table where status has changed (since we only write on updates when column value changes) and we get current state from live table. Then we select out of this set by use a LAG 1 and order by the dt_info column for each account_id. This ends up putting the previous dt_info column in the row after it historically - which is what tells us exactly when we went to that state.</p> <p>Reference : <a href="http://www.postgresql.org/docs/9.3/static/functions-window.html">Postgresql 9.3 Window Functions</a></p>]]></content><author><name></name></author><category term="SQL"/><summary type="html"><![CDATA[Using Lag and window operations with Postgresql]]></summary></entry><entry><title type="html">More Atom and Ruby - Using ruby-test</title><link href="https://fmcgeough.github.io/blog/2015/more-atom-and-ruby-using-ruby-test/" rel="alternate" type="text/html" title="More Atom and Ruby - Using ruby-test"/><published>2015-01-14T08:53:13+00:00</published><updated>2015-01-14T08:53:13+00:00</updated><id>https://fmcgeough.github.io/blog/2015/more-atom-and-ruby-using-ruby-test</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2015/more-atom-and-ruby-using-ruby-test/"><![CDATA[<p>I continue to try and get <a href="https://atom.io/">Atom</a> setup in the most productive way - especially for Ruby programming. I continue to be impressed with the tool. For unit testing I favor <a href="https://github.com/seattlerb/minitest">MiniTest</a> and I wanted a way to run tests from inside Atom without leaving the editor environment.</p> <p>After trying out of a couple of packages I settled on <a href="https://atom.io/packages/ruby-test">ruby-test</a>.</p> <h2 id="installation">Installation</h2> <p>Installation of ruby-test is straightforward. I used the command line Atom install tool <code class="language-plaintext highlighter-rouge">apm</code>. The following worked for me :</p> <p>apm install ruby-test</p> <p>After installing the ruby-test package restart Atom by using <code class="language-plaintext highlighter-rouge">atom</code> from the command line and then configure the testing package by going to Atom/Preferences/Packages and then searching for ruby-test. You’ll want to set your “Test Framework” as explained in the configuration panel.</p> <p>The tool worked seamlessly for me after doing this.</p> <h2 id="review">Review</h2> <p>Install was straightforward and the tool within <a href="https://atom.io/">Atom</a> worked seamlessly for me. My environment is :</p> <ul> <li>OS X 10.10.5 (Yosemite)</li> <li>Atom 1.2.0</li> <li>ruby-test 0.9.16</li> </ul> <p>The tool pops up a panel when running tests and shows the MiniTest output just like you’d see from the command line. The panel has a close button (unlike some of the other unit test packages I tried out in Atom) so you can close the unit test info when you no longer need it or want additional screen real estate. The panel that pops up includes a link in the left hand corner to “Settings” that brings up you to the ruby-test Settings page.</p> <p>I generally run a single test after I finish writing it by placing my cursor on the test and hitting command-control-R and then run all the tests in a Ruby test file by hitting command-control-T. Both of these approaches worked without issue.</p>]]></content><author><name></name></author><category term="Ruby"/><category term="Ruby"/><summary type="html"><![CDATA[Using Ruby with the Atom editor]]></summary></entry><entry><title type="html">George Washington and Torture</title><link href="https://fmcgeough.github.io/blog/2014/george-washington-and-torture/" rel="alternate" type="text/html" title="George Washington and Torture"/><published>2014-12-17T01:00:00+00:00</published><updated>2014-12-17T01:00:00+00:00</updated><id>https://fmcgeough.github.io/blog/2014/george-washington-and-torture</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/george-washington-and-torture/"><![CDATA[<p>“Should any American soldier be so base and infamous as to injure any [prisoner]. . . I do most earnestly enjoin you to bring him to such severe and exemplary punishment as the enormity of the crime may require. Should it extend to death itself, it will not be disproportional to its guilt at such a time and in such a cause… for by such conduct they bring shame, disgrace and ruin to themselves and their country.” – George Washington</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Historical Quotes]]></summary></entry><entry><title type="html">Postgresql - Optimizing SQL Performance</title><link href="https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance/" rel="alternate" type="text/html" title="Postgresql - Optimizing SQL Performance"/><published>2014-08-31T18:08:30+00:00</published><updated>2014-08-31T18:08:30+00:00</updated><id>https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/postgresql-optimizing-sql-performance/"><![CDATA[<p>In a standard smallish Postgresql installation its actually fairly straightforward to figure out what indexes to create to eliminate sequential scans and improve your performance. But in a larger system where there are thousands of different queries, perhaps written by dozens of different engineers, the problem of addressing performance issues gets a bit more difficult. However, the same techniques can be used on both systems. I’ll describe what I use and perhaps it will be useful for someone else.</p> <h2 id="pgbadger">pgbadger</h2> <p>pgbadger is a <a href="https://github.com/dalibo/pgbadger">Postgresql log analyzer</a>. You should have it installed and setup a cron job to analyze your log every hour and send you a report. This is sort of a default thing to do on any Postgresql installation. It gives some nice general information.</p> <p>In order to make use of it you’ll need to setup your Postgresql installation to actually log. There are settings in postgresql.conf that will need to be configured in the “ERROR REPORTING AND LOGGING” section. Here is what I use :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>log_destination = 'stderr'
logging_collector = on
log_filename = 'postgresql-%Y%m%d-%H.log'
log_truncate_on_rotation = on
log_rotation_age = 60min
log_rotation_size = 0
log_directory = '/postgres/tracelogs/pgsql'
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d %h '
</code></pre></div></div> <p>I use pgbadger from a shell script (badger_reporting.sh) that has a single line in it :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># have pgbadger parse our Postgresql log
/usr/local/bin/pgbadger --prefix '%t [%p]: [%l-1] user=%u,db=%d %h ' --outfile $@
#
</code></pre></div></div> <p>Note: you’ll want the line you pass into pgbadger to mirror the output defined in your <code class="language-plaintext highlighter-rouge">log_line_prefix</code> definition. You are giving pgbadger the information that it needs to properly parse your log file.</p> <p>This shell script is called from another that passes in the name of the file to generate and the log file as source like this :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>badger_reporting.sh $LOCAL_HTML_OUTFILE $LOCAL_LOG_FILE
</code></pre></div></div> <p>Since we’re going to send an email to ourselves with our cron job its also nice to grep the postgresql log to see if there are ERRORS in it and send those in the body of the email so its immediately obvious. I do this with :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>echo "Errors in log" &gt; $LOCAL_LOG_FILE".err"
grep "ERROR:" $LOCAL_LOG_FILE &gt;&gt; $LOCAL_LOG_FILE".err"
</code></pre></div></div> <p>The mailing on our system is handled by mutt.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># mail out the pgbadger output
cat $LOCAL_LOG_FILE".err" | mutt -s "$SUBJECT" -a "$LOCAL_HTML_OUTFILE" -- "$RECIP_LIST"
#
</code></pre></div></div> <p>There are a lot of sections in the pgbadger report. The top of the report has “Overall Statistics”. You might want to familiarize yourself with the numbers that show up here and look for things that are out of the ordinary for the same day and time in your system. I also tend to look at “Queries that took up the most time” section.</p> <p>Anyway, pgbadger is a useful tool. Its free and fairly easy to setup. On most systems it provides at least some insight.</p> <p>You should realize that on most systems you’ll also have to set the <code class="language-plaintext highlighter-rouge">log_min_duration_statement</code> to some value that keeps your log from exploding in size. This means that pgbadger will have a whole slew of statements (ordinarily the bulk of them) that it won’t know anything about. Keep this in mind. There are other tools you can add to your arsenal that provide more full insight including the one I’ll describe next.</p> <h2 id="pg-stat-statements">pg stat statements</h2> <p>This should be required for any production Postgresql system (it actually is turned on if you use Amazon RDS version of Postgresql). If you come to a new system that doesn’t have it loaded this should be one of the first things that you address. While you’re at it add auto_explain to the set of things that get pre-loaded by postgresql. This is done by editing your postgresq.conf file and doing this :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># modify what preload libraries Postgresql uses
shared_preload_libraries = 'auto_explain,pg_stat_statements' # (change requires restart)
</code></pre></div></div> <p>As noted in the awesome postgresql.conf doc that accompanies the file, a change to this line requires a Postgresql restart. Do it. Then run the following from psql :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create extension pg_stat_statements;
</code></pre></div></div> <p>You are now much more awesome. As noted on the Postgresql online doc : The <code class="language-plaintext highlighter-rouge">pg_stat_statements</code> module provides a means for tracking execution statistics of all SQL statements executed by a server. In a sense this module is like a super-sized pgbadger. It gives you information that is immensely valuable. Read all about it on web at : <a href="http://www.postgresql.org/docs/9.3/static/pgstatstatements.html">pg_stat_statements</a> (change URL for whatever version you happen to be using).</p> <p>The default number of statements that are tracked is 1,000. Personally I’d bump that up to 10x by setting the following parameter in postgresql.conf file.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pg_stat_statements.max = 10000
</code></pre></div></div> <p>But this is really dependent upon your system and how many different types of queries you have hitting your database.</p> <p>The important queries to know about <code class="language-plaintext highlighter-rouge">pg_stat_statements</code> are :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-- reset statements. Empty everything out and start regathering
SELECT pg_stat_statements_reset();

-- which queries were called the most
SELECT * FROM pg_stat_statements ORDER BY calls desc LIMIT 50;

-- which queries used the most CPU time
SELECT * FROM pg_stat_statements ORDER BY total_time desc LIMIT 50;

-- what are the top 50 queries that I should look at (by total time hit)?
SELECT query,
calls,
total_time,
rows,
100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,
ROUND((total_time::numeric / calls::numeric),2) as average_time_per_call
FROM pg_stat_statements
ORDER BY total_time
DESC LIMIT 50;

-- what are the top 50 queries that I should look at (by speed of execution) ?
SELECT query,
calls,
total_time,
rows,
100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent,
ROUND((total_time::numeric / calls::numeric),2) as average_time_per_call
FROM pg_stat_statements
ORDER BY 6
DESC LIMIT 50;
</code></pre></div></div> <p>How often should you run : <code class="language-plaintext highlighter-rouge">pg_stat_statements_reset</code>? Its really totally dependent upon your system. Here’s what I do. Every hour I grab the contents of pg_stat_statements and mail it to myself. Then I run pg_stat_statements_reset. This gives me a history and an hourly view of what is going on in the database.</p> <h2 id="pg-stat-user-tables">pg stat user tables</h2> <p>The <code class="language-plaintext highlighter-rouge">pg_stat_user_tables</code> contains a wealth of statistical information at a low level about your app tables. It is where to go to see if you are consistently sequentially scanning your million row table (whoops!) just because you missed adding an index after an application query change. Since its a running total I recommend taking a snapshot of the data, storing it in a temporary table and then taking another snapshot and diff’ing the two in order to see what happened over the period of time between your two snapshots. I actually have our system setup so this happens every hour during weekdays and business hours. I keep about a month worth of these snapshots in the temporary table. This lets me easily go back to a previous week (or two) and see if today’s traffic is significantly different from its corresponding traffic on same day at a point in the past.</p> <p>To create the temporary table perform the following SQL :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>create table public.history_of_pg_stat as
SELECT CURRENT_TIMESTAMP as time_captured, *
FROM pg_stat_user_tables;
</code></pre></div></div> <p>Then periodically do the following :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>insert into public.history_of_pg_stat
SELECT CURRENT_TIMESTAMP, *
FROM pg_stat_user_tables;
</code></pre></div></div> <p>Now you can diff the two captured rows by :</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>with most_recent_time as
(
SELECT time_captured
FROM public.history_of_pg_stat
ORDER BY 1 DESC LIMIT 1
)
,
second_time as
(
SELECT time_captured
FROM public.history_of_pg_stat
WHERE time_captured &lt; (SELECT time_captured FROM most_recent_time)
ORDER BY 1 DESC
LIMIT 1
)
,
hops2 as
(
SELECT * FROM public.history_of_pg_stat
WHERE time_captured = (SELECT time_captured FROM most_recent_time)
),
stats as
(
SELECT to_char(hops.time_captured at TIME ZONE 'EST5DT', 'HH24:MI:SS') || ' - ' || to_char(hops2.time_captured at TIME ZONE 'EST5DT' , 'HH24:MI:SS') as capture_times,
hops2.time_captured-hops.time_captured as time_range,
hops2.schemaname || '.' || hops2.relname as tablename,
hops2.seq_scan - hops.seq_scan as seq_scans,
hops2.seq_tup_read - hops.seq_tup_read as seq_tup_read,
hops2.idx_scan - hops.idx_scan as idx_scans,
hops2.idx_tup_fetch - hops.idx_tup_fetch as idx_tup_fetch,
hops2.n_tup_ins - hops.n_tup_ins as n_tup_ins,
hops2.n_tup_upd - hops.n_tup_upd as n_tup_upd,
hops2.n_live_tup - hops.n_live_tup as n_new_live_tups,
hops2.n_live_tup as current_live_tup
FROM public.history_of_pg_stat hops
JOIN hops2 ON (hops.schemaname = hops2.schemaname and hops.relname = hops2.relname)
WHERE hops.time_captured = (SELECT time_captured FROM second_time)
)
SELECT * FROM stats
WHERE (seq_scans &gt; 0 OR idx_scans &gt; 0)
AND tablename NOT LIKE 'public.history_of_pg_stat'
AND current_live_tup &gt; 5000
ORDER BY 5 DESC;
</code></pre></div></div> <p>I hope some of this sketched out advice helps someone else in tracking down performance problems or better administer their system. Happy hunting!</p>]]></content><author><name></name></author><category term="SQL"/><category term="postgresql"/><category term="SQL"/><category term="postgresql"/><summary type="html"><![CDATA[SQL Optimization with Postgresql]]></summary></entry><entry><title type="html">Postgresql Connection Info</title><link href="https://fmcgeough.github.io/blog/2014/postgresql-connection-info/" rel="alternate" type="text/html" title="Postgresql Connection Info"/><published>2014-04-22T15:36:50+00:00</published><updated>2014-04-22T15:36:50+00:00</updated><id>https://fmcgeough.github.io/blog/2014/postgresql-connection-info</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/postgresql-connection-info/"><![CDATA[<p>Some days you just want to know who is connecting up to your database. The following SQL works for 9.3 Postgresql.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SELECT COUNT(1) as num_connections,
COUNT(CASE WHEN s."state" = 'idle' THEN 1 ELSE NULL END) as num_idle_connections,
COUNT(CASE WHEN s."state" = 'idle in transaction' THEN 1 ELSE NULL END) as num_idle_in_tx_connections,
max(CASE WHEN s."state" = 'idle in transaction' THEN GREATEST(now(),s.query_start)-s.query_start ELSE NULL END) as age_of_oldest_tx,
s.datname,
s.client_addr
FROM pg_stat_activity s
WHERE s.pid != pg_backend_pid()
and s.client_addr is not null
GROUP BY s.client_addr, s.datname
ORDER BY 1 DESC;
</code></pre></div></div> <p>This query shows you total connections from by ip address along with some info that I find helpful - namely how many connections are connected but idle and how many are idle in transactions (and if so for how long).</p>]]></content><author><name></name></author><category term="database"/><category term="postgresql"/><summary type="html"><![CDATA[Who is connected to my Postgres database?]]></summary></entry><entry><title type="html">Moroccan Style Brisket</title><link href="https://fmcgeough.github.io/blog/2014/moroccan-style-brisket/" rel="alternate" type="text/html" title="Moroccan Style Brisket"/><published>2014-04-17T08:53:13+00:00</published><updated>2014-04-17T08:53:13+00:00</updated><id>https://fmcgeough.github.io/blog/2014/moroccan-style-brisket</id><content type="html" xml:base="https://fmcgeough.github.io/blog/2014/moroccan-style-brisket/"><![CDATA[<p>Frankly I don’t think they necessarily have brisket in Morocco. Never been. and Moroccan-style is probably a stretch too but I had to call it something. Ah well. It was quite yummy and I got asked for recipe. Ingredients hmmm…let’s see.</p> <ul> <li>about 2 pound brisket</li> <li>one large red onion, sliced thin</li> <li>3 carrots, cleaned &amp; chopped</li> <li>2 parsnips, cleaned &amp; chopped</li> <li>2 TBL tomato paste</li> <li>1 TBL Tukas hot pepper paste</li> <li>1 tsp sweet paprika</li> <li>1 tsp ground cumin</li> <li>1/2 tsp ground ginger</li> <li>1/4 tsp cinnamon</li> <li>1 can chickpeas (drained and rinsed)</li> <li>2-3 cups beef broth</li> <li>couple handfuls of prunes cut in half</li> <li>olive oil</li> <li>salt &amp; pepper</li> <li>slivered almonds</li> </ul> <p>Now… what did I do?</p> <p>I turned my oven on low (300 degrees).</p> <p>I got a good sized Dutch oven (3 1/2 quart I think) and added a generous amount of olive oil over medium-high heat. I added the red onions and cooked them for about 6-7 minutes until they were quite soft. I added the spices and tomato paste and Tukas hot pepper paste and cooked it for about a minute. Then I threw in the brisket, followed by all the other ingredients (except for slivered almonds) and covered the dutch oven and put it in oven for about 3 hours.</p> <p>The house smelled wonderful at the end. I heated a bit more olive oil in a non-stick pan and threw in the slivered almonds with some sea salt. Toasted them for a couple minutes.</p> <p>Slice the brisket. Cover it with a bit of the veggies and delicious broth, sprinkle with slivered almonds. There ya go.</p>]]></content><author><name></name></author><category term="food"/><category term="recipes"/><summary type="html"><![CDATA[Brisket Recipe]]></summary></entry></feed>